#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•AI Research ScraperæŠ€èƒ½çš„åŸºæœ¬åŠŸèƒ½
"""

import sys
import os

# æ£€æŸ¥æŠ€èƒ½æ–‡ä»¶æ˜¯å¦å®Œæ•´
def test_skill_files():
    print("=== æµ‹è¯•æŠ€èƒ½æ–‡ä»¶å®Œæ•´æ€§ ===\n")
    
    skill_dir = '/root/.openclaw/workspace/skills/ai-research-scraper'
    required_files = [
        'SKILL.md',
        'scripts/scraper.py',
        'references/websites.txt',
        'references/api_reference.md'
    ]
    
    all_exist = True
    for file_path in required_files:
        full_path = os.path.join(skill_dir, file_path)
        if os.path.exists(full_path):
            print(f"âœ… {file_path} å­˜åœ¨")
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å†…å®¹
            if os.path.getsize(full_path) > 0:
                print(f"   å¤§å°: {os.path.getsize(full_path)} å­—èŠ‚")
            else:
                print(f"âš ï¸  {file_path} æ˜¯ç©ºæ–‡ä»¶")
        else:
            print(f"âŒ {file_path} ä¸å­˜åœ¨")
            all_exist = False
    
    print()
    return all_exist


# æ£€æŸ¥è„šæœ¬æ˜¯å¦å¯ä»¥æ­£å¸¸å¯¼å…¥å’Œè¿è¡Œ
def test_script_import():
    print("=== æµ‹è¯•è„šæœ¬å¯¼å…¥å’ŒåŸºæœ¬åŠŸèƒ½ ===\n")
    
    try:
        sys.path.append('/root/.openclaw/workspace/skills/ai-research-scraper/scripts')
        import scraper
        
        print("âœ… è„šæœ¬å¯¼å…¥æˆåŠŸ")
        print()
        
        # æµ‹è¯•é…ç½®åŠ è½½
        print("ğŸ“„ é…ç½®æµ‹è¯•:")
        websites = scraper.read_websites()
        print(f"   ç½‘ç«™åˆ—è¡¨ä¸­æœ‰ {len(websites)} ä¸ªç½‘ç«™")
        
        for site in websites:
            print(f"   - {site['name']} ({site['url']})")
        
        # æµ‹è¯•æ–‡æœ¬å¤„ç†å‡½æ•°
        print()
        print("ğŸ“ æ–‡æœ¬å¤„ç†æµ‹è¯•:")
        test_text = "OpenAI has announced a new feature for their platform that allows users to customize chatbots with specific knowledge bases. This update will make it easier for developers to create specialized AI assistants for various applications."
        
        # æµ‹è¯•å…³é”®è¯æ£€æµ‹
        is_product = scraper.is_related_to_product_development("New Feature Announcement", test_text)
        print(f"   å…³é”®è¯æ£€æµ‹: {'âœ… åŒ…å«äº§å“ç›¸å…³å…³é”®è¯' if is_product else 'âŒ ä¸åŒ…å«'}")
        
        # æµ‹è¯•æ–‡æœ¬æˆªæ–­
        truncated = scraper.truncate_text(test_text, 20)
        print(f"   æ–‡æœ¬æˆªæ–­ (20ä¸ªè¯): '{truncated}'")
        
        return True
        
    except Exception as e:
        print(f"âŒ è„šæœ¬å¯¼å…¥å¤±è´¥: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return False


# æµ‹è¯•ç½‘ç«™åˆ—è¡¨
def test_websites_list():
    print("\n=== æµ‹è¯•ç½‘ç«™åˆ—è¡¨ ===\n")
    
    websites = [
        {
            'name': 'TechCrunch AI',
            'url': 'https://techcrunch.com/ai/',
            'rss': 'https://techcrunch.com/ai/feed/'
        },
        {
            'name': 'VentureBeat AI',
            'url': 'https://venturebeat.com/ai/',
            'rss': 'https://venturebeat.com/ai/feed/'
        },
        {
            'name': 'MIT Tech Review AI',
            'url': 'https://www.technologyreview.com/tag/artificial-intelligence/',
            'rss': 'https://www.technologyreview.com/feed/tag/artificial-intelligence/'
        },
        {
            'name': 'OpenAI Blog',
            'url': 'https://openai.com/blog',
            'rss': 'https://openai.com/blog/rss'
        },
        {
            'name': 'Google AI Blog',
            'url': 'https://ai.googleblog.com/',
            'rss': 'https://ai.googleblog.com/feeds/posts/default'
        },
        {
            'name': 'Microsoft AI Blog',
            'url': 'https://blogs.microsoft.com/ai/',
            'rss': 'https://blogs.microsoft.com/ai/feed/'
        },
        {
            'name': 'NVIDIA Blog',
            'url': 'https://blogs.nvidia.com/blog/category/ai/',
            'rss': 'https://blogs.nvidia.com/blog/category/ai/feed/'
        },
        {
            'name': 'Medium AI Articles',
            'url': 'https://medium.com/tag/artificial-intelligence',
            'rss': 'https://medium.com/feed/tag/artificial-intelligence'
        }
    ]
    
    print("âœ… ç½‘ç«™åˆ—è¡¨åŒ…å«ä»¥ä¸‹AIé¢†åŸŸçš„çŸ¥åç½‘ç«™:")
    for site in websites:
        print(f"   - {site['name']}")
    
    print(f"\nğŸ¯ å…± {len(websites)} ä¸ªç½‘ç«™")
    
    return True


def main():
    print("AI Research Scraper æŠ€èƒ½æµ‹è¯•\n")
    print("=" * 50)
    
    all_tests_pass = True
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    if not test_skill_files():
        all_tests_pass = False
    
    if not test_script_import():
        all_tests_pass = False
    
    if not test_websites_list():
        all_tests_pass = False
    
    print("\n" + "=" * 50)
    
    if all_tests_pass:
        print("\nâœ… æŠ€èƒ½æµ‹è¯•é€šè¿‡")
        print("\nğŸ“‹ ä½¿ç”¨è¯´æ˜:")
        print("   è¿è¡ŒæŠ€èƒ½: python3 scripts/scraper.py --max-tokens 300")
        print("   è‡ªå®šä¹‰ç½‘ç«™: ç¼–è¾‘ references/websites.txt")
        print("   é…ç½®: æŸ¥çœ‹ references/api_reference.md")
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
    
    return all_tests_pass


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
